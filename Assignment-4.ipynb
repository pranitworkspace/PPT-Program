{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8efc20de",
   "metadata": {},
   "source": [
    "General Linear Model:\n",
    "\n",
    "1. What is the purpose of the General Linear Model (GLM)?\n",
    "2. What are the key assumptions of the General Linear Model?\n",
    "3. How do you interpret the coefficients in a GLM?\n",
    "4. What is the difference between a univariate and multivariate GLM?\n",
    "5. Explain the concept of interaction effects in a GLM.\n",
    "6. How do you handle categorical predictors in a GLM?\n",
    "7. What is the purpose of the design matrix in a GLM?\n",
    "8. How do you test the significance of predictors in a GLM?\n",
    "9. What is the difference between Type I, Type II, and Type III sums of squares in a GLM?\n",
    "10. Explain the concept of deviance in a GLM.\n",
    "\n",
    "Regression:\n",
    "\n",
    "11. What is regression analysis and what is its purpose?\n",
    "12. What is the difference between simple linear regression and multiple linear regression?\n",
    "13. How do you interpret the R-squared value in regression?\n",
    "14. What is the difference between correlation and regression?\n",
    "15. What is the difference between the coefficients and the intercept in regression?\n",
    "16. How do you handle outliers in regression analysis?\n",
    "17. What is the difference between ridge regression and ordinary least squares regression?\n",
    "18. What is heteroscedasticity in regression and how does it affect the model?\n",
    "19. How do you handle multicollinearity in regression analysis?\n",
    "20. What is polynomial regression and when is it used?\n",
    "\n",
    "Loss function:\n",
    "\n",
    "21. What is a loss function and what is its purpose in machine learning?\n",
    "22. What is the difference between a convex and non-convex loss function?\n",
    "23. What is mean squared error (MSE) and how is it calculated?\n",
    "24. What is mean absolute error (MAE) and how is it calculated?\n",
    "25. What is log loss (cross-entropy loss) and how is it calculated?\n",
    "26. How do you choose the appropriate loss function for a given problem?\n",
    "27. Explain the concept of regularization in the context of loss functions.\n",
    "28. What is Huber loss and how does it handle outliers?\n",
    "29. What is quantile loss and when is it used?\n",
    "30. What is the difference between squared loss and absolute loss?\n",
    "\n",
    "Optimizer (GD):\n",
    "\n",
    "31. What is an optimizer and what is its purpose in machine learning?\n",
    "32. What is Gradient Descent (GD) and how does it work?\n",
    "33. What are the different variations of Gradient Descent?\n",
    "34. What is the learning rate in GD and how do you choose an appropriate value?\n",
    "35. How does GD handle local optima in optimization problems?\n",
    "36. What is Stochastic Gradient Descent (SGD) and how does it differ from GD?\n",
    "37. Explain the concept of batch size in GD and its impact on training.\n",
    "38. What is the role of momentum in optimization algorithms?\n",
    "39. What is the difference between batch GD, mini-batch GD, and SGD?\n",
    "40. How does the learning rate affect the convergence of GD?\n",
    "\n",
    "Regularization:\n",
    "\n",
    "41. What is regularization and why is it used in machine learning?\n",
    "42. What is the difference between L1 and L2 regularization?\n",
    "43. Explain the concept of ridge regression and its role in regularization.\n",
    "44. What is the elastic net regularization and how does it combine L1 and L2 penalties?\n",
    "45. How does regularization help prevent overfitting in machine learning models?\n",
    "46. What is early stopping and how does it relate to regularization?\n",
    "47. Explain the concept of dropout regularization in neural networks.\n",
    "48. How do you choose the regularization parameter in a model?\n",
    "49. What\n",
    "\n",
    " is the difference between feature selection and regularization?\n",
    "50. What is the trade-off between bias and variance in regularized models?\n",
    "\n",
    "SVM:\n",
    "\n",
    "51. What is Support Vector Machines (SVM) and how does it work?\n",
    "52. How does the kernel trick work in SVM?\n",
    "53. What are support vectors in SVM and why are they important?\n",
    "54. Explain the concept of the margin in SVM and its impact on model performance.\n",
    "55. How do you handle unbalanced datasets in SVM?\n",
    "56. What is the difference between linear SVM and non-linear SVM?\n",
    "57. What is the role of C-parameter in SVM and how does it affect the decision boundary?\n",
    "58. Explain the concept of slack variables in SVM.\n",
    "59. What is the difference between hard margin and soft margin in SVM?\n",
    "60. How do you interpret the coefficients in an SVM model?\n",
    "\n",
    "Decision Trees:\n",
    "\n",
    "61. What is a decision tree and how does it work?\n",
    "62. How do you make splits in a decision tree?\n",
    "63. What are impurity measures (e.g., Gini index, entropy) and how are they used in decision trees?\n",
    "64. Explain the concept of information gain in decision trees.\n",
    "65. How do you handle missing values in decision trees?\n",
    "66. What is pruning in decision trees and why is it important?\n",
    "67. What is the difference between a classification tree and a regression tree?\n",
    "68. How do you interpret the decision boundaries in a decision tree?\n",
    "69. What is the role of feature importance in decision trees?\n",
    "70. What are ensemble techniques and how are they related to decision trees?\n",
    "\n",
    "Ensemble Techniques:\n",
    "\n",
    "71. What are ensemble techniques in machine learning?\n",
    "72. What is bagging and how is it used in ensemble learning?\n",
    "73. Explain the concept of bootstrapping in bagging.\n",
    "74. What is boosting and how does it work?\n",
    "75. What is the difference between AdaBoost and Gradient Boosting?\n",
    "76. What is the purpose of random forests in ensemble learning?\n",
    "77. How do random forests handle feature importance?\n",
    "78. What is stacking in ensemble learning and how does it work?\n",
    "79. What are the advantages and disadvantages of ensemble techniques?\n",
    "80. How do you choose the optimal number of models in an ensemble?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924181e5",
   "metadata": {},
   "source": [
    "1. The General Linear Model (GLM) is a statistical framework that allows for the analysis of the relationship between a dependent variable and one or more independent variables.\n",
    "2. The key assumptions of the GLM include linearity, independence of observations, normality of residuals, equal variances, and absence of multicollinearity.\n",
    "3. In a GLM, the coefficients represent the estimated change in the dependent variable associated with a one-unit change in the corresponding independent variable, holding other variables constant.\n",
    "4. A univariate GLM involves a single dependent variable and one or more independent variables, while a multivariate GLM involves multiple dependent variables and one or more independent variables.\n",
    "5. In a GLM, interaction effects occur when the effect of one independent variable on the dependent variable depends on the level or presence of another independent variable. It signifies that the relationship between variables is not simply additive.\n",
    "6. Categorical predictors in a GLM can be handled by using dummy variables or contrast coding. This allows the categorical variables to be included as predictors in the GLM by representing them as a series of binary variables.\n",
    "7. The design matrix in a GLM is a matrix that represents the independent variables in the model. It allows for the estimation of the coefficients and the calculation of predicted values.\n",
    "8. The significance of predictors in a GLM can be tested using hypothesis tests, such as the t-test or F-test, to determine if the estimated coefficients are significantly different from zero.\n",
    "9. Type I, Type II, and Type III sums of squares are different methods of partitioning the sum of squares in a GLM to assess the contribution of each predictor variable to the model. They differ in the order in which the variables are entered into the model.\n",
    "10. Deviance in a GLM is a measure of the goodness-of-fit of the model. It quantifies the difference between the observed data and the expected data under the fitted model. Lower deviance indicates a better fit of the model to the data.\n",
    "\n",
    "Regression:\n",
    "\n",
    "11. Regression analysis is a statistical technique used to model the relationship between a dependent variable and one or more independent variables. Its purpose is to understand how changes in the independent variables are associated with changes in the dependent variable.\n",
    "12. Simple linear regression involves a single independent variable predicting a dependent variable, while multiple linear regression involves multiple independent variables predicting a dependent variable simultaneously.\n",
    "13. The R-squared value in regression represents the proportion of variance in the dependent variable that is explained by the independent variables. It ranges from 0 to 1, where 0 indicates no explanatory power and 1 indicates perfect fit.\n",
    "14. Correlation measures the strength and direction of the linear relationship between two variables, while regression aims to model and predict the value of a dependent variable based on independent variables.\n",
    "15. Coefficients in regression represent the estimated change in the dependent variable for a one-unit change in the corresponding independent variable, while the intercept represents the expected value of the dependent variable when all independent variables are zero.\n",
    "16. Outliers in regression analysis can be handled by identifying and examining their impact on the model. They can be treated by removing the outliers, transforming the data, or using robust regression methods.\n",
    "17. Ridge regression is a regularization technique that adds a penalty term to the regression objective function to reduce the impact of multicollinearity. Ordinary least squares regression does not include a penalty term.\n",
    "18. Heteroscedasticity in regression refers to the unequal variances of the residuals across the range of the independent variables. It can affect the reliability of statistical inferences and the accuracy of coefficient estimates.\n",
    "19. Multicollinearity occurs when independent variables in a regression model are highly correlated with each other. It can lead to unstable coefficient estimates and difficulties in interpreting the individual effects of the variables. It can be addressed by removing correlated variables or using regularization techniques.\n",
    "20. Polynomial regression is a regression technique that models the relationship between the independent variable and the dependent variable as an nth-degree polynomial. It is used when the relationship between the variables is nonlinear.\n",
    "\n",
    "Loss function:\n",
    "\n",
    "21. A loss function is a measure of how well a machine learning model predicts the target variable. It quantifies the difference between the predicted values and the actual values. The purpose of a loss function is to guide the learning algorithm in finding the optimal model parameters.\n",
    "22. A convex loss function has a single minimum point, making it easier to optimize, while a non-convex loss function may have multiple local minima, making optimization more challenging.\n",
    "23. Mean Squared Error (MSE) is a loss function commonly used in regression problems. It is calculated as the average squared difference between the predicted values and the actual values.\n",
    "24. Mean Absolute Error (MAE) is a loss function similar to MSE, but it calculates the average absolute difference between the predicted values and the actual values.\n",
    "25. Log Loss, also known as cross-entropy loss, is a loss function used in classification problems. It measures the performance of a classification model by penalizing incorrect predictions based on the logarithm of the predicted probabilities.\n",
    "26. The choice of an appropriate loss function depends on the specific problem and the desired properties of the model. For example, MSE may be suitable for problems where large errors are more important, while MAE may be preferred when the outliers have a significant impact.\n",
    "27. Regularization in the context of loss functions is a technique used to prevent overfitting by adding a penalty term to the loss function. Regularization encourages simpler models by penalizing large coefficients and reducing model complexity.\n",
    "28. Huber loss is a loss function that combines the properties of squared loss (MSE) and absolute loss (MAE). It is less sensitive to outliers and provides a balance between robustness and smoothness.\n",
    "29. Quantile loss is a loss function used in quantile regression, which aims to estimate the conditional quantiles of the dependent variable. It measures the deviation between the predicted quantiles and the actual quantiles.\n",
    "30. Squared loss penalizes larger errors more than absolute loss\n",
    "\n",
    ". It magnifies the impact of outliers, while absolute loss treats all errors equally regardless of their magnitude.\n",
    "\n",
    "Optimizer (GD):\n",
    "\n",
    "31. An optimizer is an algorithm or method used to minimize the loss function and find the optimal values of the model parameters. It adjusts the parameters iteratively based on the gradients of the loss function with respect to the parameters.\n",
    "32. Gradient Descent (GD) is an optimization algorithm used to find the minimum of a function by iteratively updating the parameters in the direction of the negative gradient. It starts with an initial guess and takes steps proportional to the negative of the gradient until it reaches a minimum.\n",
    "33. Variations of Gradient Descent include Batch Gradient Descent, Mini-Batch Gradient Descent, and Stochastic Gradient Descent. Batch GD uses the entire training dataset to update the parameters, while Mini-Batch GD uses a small randomly sampled subset, and Stochastic GD uses a single randomly chosen sample.\n",
    "34. The learning rate in GD determines the step size taken in each iteration to update the parameters. Choosing an appropriate learning rate is crucial, as a too large value may cause the algorithm to diverge, while a too small value may result in slow convergence.\n",
    "35. GD can handle local optima by using random initialization of parameters, exploring different paths in the parameter space, and using techniques like momentum or adaptive learning rates.\n",
    "36. Stochastic Gradient Descent (SGD) is a variation of GD that uses a single randomly chosen sample at each iteration to update the parameters. It is computationally efficient for large datasets but may exhibit more noise in the parameter updates.\n",
    "37. Batch size in GD refers to the number of samples used in each iteration to update the parameters. A larger batch size provides a more accurate estimate of the gradient but requires more memory and computational resources.\n",
    "38. Momentum in optimization algorithms helps accelerate the convergence by accumulating the past gradients and smoothing the parameter updates. It allows the optimizer to move more consistently in the parameter space, especially in the presence of noisy gradients.\n",
    "39. Batch GD uses the entire training dataset in each iteration, Mini-Batch GD uses a randomly sampled subset, and SGD uses a single randomly chosen sample. The choice depends on the computational resources available and the trade-off between accuracy and speed.\n",
    "40. The learning rate affects the convergence of GD by determining the step size taken in each iteration. A larger learning rate may cause overshooting or divergence, while a smaller learning rate may result in slower convergence.\n",
    "\n",
    "Regularization:\n",
    "\n",
    "41. Regularization is a technique used in machine learning to prevent overfitting and improve the generalization of a model. It introduces additional constraints or penalties to the loss function to discourage complex models with high variance.\n",
    "42. L1 and L2 regularization are two common regularization techniques. L1 regularization adds a penalty term proportional to the absolute value of the coefficients, promoting sparsity and feature selection. L2 regularization adds a penalty term proportional to the square of the coefficients, encouraging smaller and more evenly distributed coefficients.\n",
    "43. Ridge regression is a linear regression technique that uses L2 regularization to mitigate the impact of multicollinearity. It adds a penalty term to the loss function, resulting in a trade-off between the goodness-of-fit and the complexity of the model.\n",
    "44. Elastic Net regularization combines L1 and L2 penalties to strike a balance between feature selection and coefficient shrinkage. It adds a linear combination of L1 and L2 penalty terms to the loss function, allowing for simultaneous variable selection and coefficient estimation.\n",
    "45. Regularization helps prevent overfitting by reducing the model's reliance on individual data points and reducing the complexity of the model. It adds a bias to the estimates but reduces the variance, leading to improved generalization performance.\n",
    "46. Early stopping is a form of regularization where the training of the model is stopped early based on the performance on a validation set. It helps prevent overfitting by monitoring the validation loss and terminating training when the loss starts to increase.\n",
    "47. Dropout regularization is a technique used in neural networks to reduce overfitting. It randomly sets a fraction of the input units to zero during each training iteration, forcing the network to learn more robust and generalizable features.\n",
    "48. The regularization parameter in a model determines the strength of the penalty term in the loss function. It controls the trade-off between fitting the training data and reducing model complexity. The optimal value is typically determined through cross-validation or other model selection techniques.\n",
    "49. Feature selection involves choosing a subset of relevant features, while regularization imposes constraints on the model to control the complexity and avoid overfitting. Regularization can perform feature selection as a byproduct, but feature selection techniques can be more focused and flexible.\n",
    "50. The bias-variance trade-off refers to the trade-off between the model's ability to capture the true underlying relationship (low bias) and its sensitivity to the training data (low variance). Regularized models strike a balance by reducing variance at the cost of a slightly increased bias.\n",
    "\n",
    "SVM:\n",
    "\n",
    "51. Support Vector Machines (SVM) is a supervised machine learning algorithm used for classification and regression. It finds an optimal hyperplane that maximally separates the data points in different classes.\n",
    "\n",
    "\n",
    "52. The kernel trick in SVM is a technique that allows the algorithm to operate in a higher-dimensional feature space without explicitly calculating the transformed features. It enables SVM to learn nonlinear decision boundaries by implicitly mapping the data into a higher-dimensional space.\n",
    "53. Support vectors in SVM are the data points that lie closest to the decision boundary. They play a crucial role in defining the decision boundary and determining the margin.\n",
    "54. The margin in SVM is the region separating the support vectors from the decision boundary. It is the distance between the decision boundary and the closest data points. A larger margin indicates a more robust and generalizable model.\n",
    "55. Unbalanced datasets in SVM can be handled by adjusting the class weights or using techniques like undersampling or oversampling to balance the class distribution. Additionally, using appropriate evaluation metrics that account for class imbalance is important.\n",
    "56. Linear SVM uses a linear decision boundary to separate the data, while non-linear SVM uses a nonlinear decision boundary achieved through the use of kernel functions.\n",
    "57. The C-parameter in SVM controls the trade-off between maximizing the margin and minimizing the training errors. A larger C-value allows for more training errors but a narrower margin, while a smaller C-value emphasizes a wider margin with fewer training errors.\n",
    "58. Slack variables in SVM are introduced to handle cases where the data points are not linearly separable. They allow for some misclassification of training examples while still trying to maximize the margin.\n",
    "59. Hard margin SVM aims to find a decision boundary that perfectly separates the data points. Soft margin SVM allows for some misclassification by introducing slack variables, which enables the model to handle noisy or overlapping data.\n",
    "60. In an SVM model, the coefficients represent the weights assigned to the features, indicating their importance in the decision boundary. The interpretation of coefficients depends on the specific kernel used and the scaling of the features.\n",
    "\n",
    "Decision Trees:\n",
    "\n",
    "61. A decision tree is a supervised machine learning algorithm that predicts the value of a target variable by learning simple decision rules inferred from the data features. It uses a tree-like model of decisions and their possible consequences.\n",
    "62. Splits in a decision tree are made based on specific criteria, such as Gini index or entropy, to maximize the information gain at each level. The goal is to create homogeneous subsets of data in terms of the target variable within each branch of the tree.\n",
    "63. Impurity measures, such as Gini index and entropy, quantify the impurity or disorder within a set of samples. They are used to evaluate the quality of a split and choose the best attribute for branching in a decision tree.\n",
    "64. Information gain in decision trees measures the reduction in entropy or Gini index achieved by splitting the data based on a specific attribute. It quantifies the usefulness of an attribute in improving the homogeneity of the subsets.\n",
    "65. Missing values in decision trees can be handled by either excluding the samples with missing values or imputing them with estimated values based on other available information.\n",
    "66. Pruning in decision trees is a technique used to prevent overfitting by removing unnecessary branches or nodes from the tree. It improves the model's generalization by simplifying the decision rules and reducing complexity.\n",
    "67. A classification tree is used for predicting categorical or discrete target variables, while a regression tree is used for predicting continuous or numerical target variables.\n",
    "68. The decision boundaries in a decision tree can be interpreted as the threshold values for the features that determine the branching paths. Each region within the decision boundaries represents a predicted class or value.\n",
    "69. Feature importance in decision trees quantifies the relative importance of each feature in the prediction process. It is calculated based on the reduction in impurity or the information gain achieved by splitting on a particular feature.\n",
    "70. Ensemble techniques combine multiple decision trees to improve prediction performance. Examples include Random Forests, Gradient Boosting, and AdaBoost. Ensemble techniques can provide better generalization and robustness by reducing variance and addressing overfitting.\n",
    "\n",
    "Ensemble Techniques:\n",
    "\n",
    "71. Ensemble techniques in machine learning combine multiple individual models to make predictions. They aim to improve prediction accuracy, reduce overfitting, and increase model robustness.\n",
    "72. Bagging (Bootstrap Aggregating) is an ensemble technique that involves training multiple models on different bootstrap samples of the training data. It aggregates the predictions of individual models to make the final prediction.\n",
    "73. Bootstrapping in bagging is a technique that involves randomly sampling the training data with replacement to create multiple subsets of data. Each subset is then used to train a separate model in the ensemble.\n",
    "74. Boosting is an ensemble technique that iteratively trains weak models and focuses on samples that were previously misclassified. It combines the predictions of individual models, giving higher weight to the models that perform well on difficult samples.\n",
    "75. AdaBoost (Adaptive Boosting) is a boosting algorithm that assigns weights to the training samples based on their difficulty and focuses on the misclassified samples to train subsequent models. It combines the predictions of individual models by assigning weights to them.\n",
    "76. Random Forests is an ensemble technique that combines the predictions of multiple decision trees. Each tree is\n",
    "\n",
    " trained on a random subset of features, and the final prediction is made by majority voting or averaging the predictions of individual trees.\n",
    "77. Random forests estimate feature importance based on the reduction in impurity achieved by each feature in the ensemble of decision trees. They provide a measure of feature importance that can be used for feature selection and interpretation.\n",
    "78. Stacking in ensemble learning involves training multiple models on the same dataset and using their predictions as inputs to a meta-model, which makes the final prediction. It aims to combine the strengths of individual models.\n",
    "79. The advantages of ensemble techniques include improved prediction accuracy, increased robustness, and better generalization. However, they may be more computationally expensive and complex compared to individual models.\n",
    "80. The optimal number of models in an ensemble depends on various factors such as the size and quality of the training data, the complexity of the problem, and the trade-off between computational resources and performance. It is often determined through cross-validation or other model selection techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de247add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11c080",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c14b19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d911858a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c373531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7a3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f752333f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a3913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4a84ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d32e80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c711e4d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0526278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6453f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b687c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276e6d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180a98d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
